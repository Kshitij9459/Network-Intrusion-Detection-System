# -*- coding: utf-8 -*-
"""Intrusion Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQUcb7PhDhbj7BVOCdl_lvwCPBseLQqZ
"""

from google.colab import drive

drive.mount('/content/gdrive')

!ls

# %cd gdrive

!ls

# %cd 'My Drive'/'Network Intrusion Detection'

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.preprocessing import normalize
import random
from collections import *
from tqdm import tqdm_notebook as tqdm

train_pd=pd.read_csv('KDDTrain+.csv',index_col=False)

train_pd1=pd.read_csv('KDDTrain1+.csv')

class_a=np.array(train_pd['class'])
class_b=np.array(train_pd1['class'])

train_pd['class'].unique()

for i in range(len(class_a)):
  
  if class_a[i] in ['back', 'land', 'neptune', 'pod','smurf','teardrop','apache2', 'udpstorm','processtable', 'worm']:
    class_b[i]='dos'
    
  elif class_a[i] in ['satan', 'ipsweep', 'nmap', 'portsweep', 'mscan','saint']:
    class_b[i]='probe'
    
  elif class_a[i] in ['guess_passwd', 'ftp_write', 'imap', 'phf','multihop', 'warezmaster', 'warezclient', 'spy']:
    class_b[i]='r2l'
    
  elif class_a[i] in ['buffer_overflow', 'loadmodule','rootkit', 'perl','sqlattack', 'xterm', 'ps']:
    class_b[i]='u2r'

del train_pd1['class']
train_pd1.insert(41,'class',class_b)
train_pd1['class'].unique()

train_pd=pd.get_dummies(train_pd)
train_pd1=pd.get_dummies(train_pd1)

train_pd.head(1)

attack_names=['back','buffer_overflow','ftp_write','guess_passwd','imap','ipsweep', 'land','loadmodule','multihop','neptune','nmap', 'normal','perl','phf','pod','portsweep','rootkit','satan', 'smurf', 'spy', 'teardrop',   'warezclient',   'warezmaster'  
         ]

train_pd1.head(1)

defend_names=['dos','normal','probe','r2l','u2r']

train_pd1=np.array(train_pd1)
train_pd=np.array(train_pd)
train_pd1[0]

train_pd1[:,:-5].shape

X_attacker=train_pd[:,:-23]
X_defender=train_pd1[:,:-5]
y_attacker=train_pd[:,-23:]
y_defender=train_pd1[:,-5:]

X_attacker=normalize(X_attacker)
X_defender=normalize(X_defender)
y_attacker=normalize(y_attacker)
y_defender=normalize(y_defender)

# ALL HYPERPARAMETERS

N=200000  #capacity of replay memory
num_episodes=1000  #number of episodes
learning_rate=0.1
discount_factor=0.001
max_time=200      #number of time steps
eps_min_atk=0.8
eps_max_atk=1
eps_min_def=0.01
eps_max_def=1
K=0.005
train_step=4
start_step=200
batch_size=32

replay_memory=deque()
global_step=0
attack_outputs=23
defend_outputs=5

def attack_network(X,scope):
  
  initializer=tf.contrib.layers.variance_scaling_initializer()
  with tf.variable_scope(scope) as scope:
    
    layer1=tf.layers.dense(inputs=X,units=100,activation='relu',kernel_initializer=initializer)
    output=tf.layers.dense(inputs=layer1,units=23,activation='sigmoid',kernel_initializer=initializer)
    
    tf.summary.histogram('attack_output',output)
    
    var = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}

    return var,output

def defend_network(X,scope):
  
  initializer=tf.contrib.layers.variance_scaling_initializer()
  with tf.variable_scope(scope) as scope:
    
    layer1=tf.layers.dense(inputs=X,units=100,kernel_initializer=initializer,activation='relu')
    layer2=tf.layers.dense(inputs=layer1,units=100,kernel_initializer=initializer,activation='relu')
    layer3=tf.layers.dense(inputs=layer2,units=100,kernel_initializer=initializer,activation='relu')
    output=tf.layers.dense(inputs=layer3,units=5,kernel_initializer=initializer,activation='sigmoid')
    
    tf.summary.histogram('defend_output',output)
    
    var = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}
    
    return var,output

def sample(attack_label):
  
  index=[]
  
  for i in range(len(y_attacker)):
    
    if y_attacker[i][attack_label]==1:
      index.append(i)
      
  return X_attacker[random.choice(index)].reshape((1,122))

def best_action_attacker(action,steps):
  
  epsilon=max(eps_min_atk,eps_max_atk-K*steps)
  
  if np.random.rand()<epsilon:
    return np.random.randint(attack_outputs)
  
  else:
    return action
  
def best_action_defender(action,steps):
  
  epsilon=max(eps_min_def,eps_max_def-K*steps)
  
  if np.random.rand()<epsilon:
    return np.random.randint(defend_outputs)
  
  else:
    return action

def agent_reward(env_action,agent_action):
  
  if attack_names[env_action] in ['back', 'land', 'neptune', 'pod','smurf','teardrop','apache2', 'udpstorm','processtable', 'worm'] and defend_names[agent_action] in ['dos']:
    return 1,0
    
  elif attack_names[env_action] in ['satan', 'ipsweep', 'nmap', 'portsweep', 'mscan','saint'] and defend_names[agent_action] in ['probe']:
    return 1,0
    
  elif attack_names[env_action] in ['guess_passwd', 'ftp_write', 'imap', 'phf','multihop', 'warezmaster', 'warezclient', 'spy'] and defend_names[agent_action] in ['r2l']:
    return 1,0
    
  elif attack_names[env_action] in ['buffer_overflow', 'loadmodule','rootkit', 'perl','sqlattack', 'xterm', 'ps'] and defend_names[agent_action] in ['u2r']:
    return 1,0
  
  elif attack_names[env_action] in ['normal'] and defend_names[agent_action] in ['normal']:
    return 1,0
  
  else:
    return 0,1

def batch_sample(batch_size):
  
  perm=np.random.permutation(len(replay_memory))[:batch_size]
  store=np.array(replay_memory)[perm]
    
  return store[:,0],store[:,1],store[:,2],store[:,3],store[:,4],store[:,5]

tf.reset_default_graph()
X=tf.placeholder(dtype=tf.float32,shape=(None,122))

main_attacker_val,main_attacker=attack_network(X,'main_attack')
target_attacker_val,target_attacker=attack_network(X,'target_attack')
main_defend_val,main_defend=defend_network(X,'main_defend')
target_defend_val,target_defend=defend_network(X,'target_defend')

#Copy weights from Main to target network
copy_op_attack = [tf.assign(target_name, main_attacker_val[var_name]) for var_name, target_name in target_attacker_val.items()]
copy_weights_attacker = tf.group(*copy_op_attack)

copy_op_defend = [tf.assign(target_name, main_defend_val[var_name]) for var_name, target_name in target_defend_val.items()]
copy_weights_defender = tf.group(*copy_op_defend)

#stores action array batch-wise
X_action=tf.placeholder(dtype=tf.int32,shape=(None,))
#computes Q(s,a) value for training
Q_action_env=tf.reduce_sum(main_attacker*tf.one_hot(X_action,attack_outputs),axis=-1,keepdims=True)
Q_action_agent=tf.reduce_sum(main_defend*tf.one_hot(X_action,defend_outputs),axis=-1,keepdims=True)

y=tf.placeholder(tf.float32,shape=(None,1))
cost_env=tf.reduce_mean(tf.square(y-Q_action_env))
cost_agent=tf.reduce_mean(tf.square(y-Q_action_agent))
optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)
train_env=optimizer.minimize(cost_env)
train_agent=optimizer.minimize(cost_agent)
init=tf.global_variables_initializer()


loss_summary_defender=tf.summary.scalar('D_LOSS',cost_agent)
merge_summary=tf.summary.merge_all()
file_writer=tf.summary.FileWriter('logs/1',tf.get_default_graph())
saver = tf.train.Saver()

with tf.Session() as sess:
  
  
  init.run()
  
  copy_weights_attacker.run()         #copy main weights of attacker to target network
  copy_weights_defender.run()         #copy main weights of defender to target network
  # iteration over the number of episodes
  for i in tqdm(range(num_episodes)):
    #Random initial state
    initial_state=X_attacker[np.random.randint(125973)].reshape(1,122)
    # Action chosen by environment according to its policy
    env_action=np.argmax(main_attacker.eval(feed_dict={X:initial_state}))
    # E-greedy choice
    
    env_action=best_action_attacker(env_action,global_step)
    
    # state chosen randomly with above intrusion label
    current_state=sample(env_action)
    # iteration over time
    for j in range(max_time):
      
      global_step+=1
      
      agent_action=np.argmax(main_defend.eval(feed_dict={X:current_state}))
      
      agent_action=best_action_defender(agent_action,global_step)
      
      reward_agent,reward_env=agent_reward(env_action,agent_action)
      
      next_env_action=np.argmax(main_attacker.eval(feed_dict={X:current_state}))
      
      next_env_action=best_action_attacker(next_env_action,global_step)
      
      next_state=sample(next_env_action)
      
      if len(replay_memory)==N:
        replay_memory.popleft()
        
      replay_memory.append([current_state[0],agent_action,env_action,next_state[0],reward_agent,reward_env])
      
      
      
      current_state=next_state
      
      env_action=next_env_action
      
      
      
      if global_step%train_step==0 and global_step>start_step:
        
        current_state_arr,agent_action_arr,env_action_arr,next_state_arr,agent_reward_arr,env_reward_arr=batch_sample(batch_size)
        
        temp=np.zeros((batch_size,122))
        for i in range(batch_size):
          temp[i]=current_state_arr[i]
          
        current_state_arr=temp
        
        for i in range(batch_size):
          temp[i]=next_state_arr[i]
        
        next_state_arr=temp
        
        y_agent=agent_reward_arr+discount_factor*np.max(target_defend.eval(feed_dict={X:next_state_arr}))
        
          
        mrg_summary=merge_summary.eval(feed_dict={X:current_state_arr,y:np.expand_dims(y_agent,axis=-1),X_action:agent_action_arr})
        file_writer.add_summary(mrg_summary, global_step)
        
        #print(next_state_arr.shape,agent_reward_arr.shape)
        
        y_agent=agent_reward_arr+discount_factor*np.max(target_defend.eval(feed_dict={X:next_state_arr}))
        
        y_env=env_reward_arr+discount_factor*np.max(target_attacker.eval(feed_dict={X:next_state_arr}))
        
        train_loss_agent,opt_agent=sess.run([cost_agent,train_agent],feed_dict={X:current_state_arr,y:np.expand_dims(y_agent,axis=-1),X_action:agent_action_arr})

        train_loss_env,opt_env=sess.run([cost_env,train_env],feed_dict={X:current_state_arr,y:np.expand_dims(y_env,axis=-1),X_action:env_action_arr})
        
      if global_step%50==0 and global_step>start_step:
        
        copy_weights_attacker.run()         #copy main weights of attacker to target network
        copy_weights_defender.run()  

  save_path = saver.save(sess, "/tmp/model.ckpt")
  print("Model saved in path: %s" % save_path)

test_pd=pd.read_csv('KDDTest+.csv',index_col=False)
test_pd1=pd.read_csv('KDDTest1+.csv',index_col=False)

class_a=np.array(test_pd['class'])
class_b=np.array(test_pd1['class'])

for i in range(len(class_a)):
  
  if class_a[i] in ['back', 'land', 'neptune', 'pod','smurf','teardrop','apache2', 'udpstorm','processtable', 'worm','mailbomb']:
    class_b[i]='dos'
    
  elif class_a[i] in ['satan', 'ipsweep', 'nmap', 'portsweep', 'mscan','saint']:
    class_b[i]='probe'
    
  elif class_a[i] in ['guess_passwd', 'ftp_write', 'imap', 'phf','multihop', 'warezmaster', 'warezclient', 'spy','xlock','xsnoop','snmpguess','snmpgetattack','httptunnel','sendmail','named']:
    class_b[i]='r2l'
    
  elif class_a[i] in ['buffer_overflow', 'loadmodule','rootkit', 'perl','sqlattack', 'xterm', 'ps']:
    class_b[i]='u2r'
    
  elif class_a[i] in ['normal']:
    class_b[i]='normal'
    
  else:
    print(class_a[i])

del test_pd1['class']
test_pd1.insert(41,'class',class_b)
test_pd1['class'].unique()

test_pd=pd.get_dummies(test_pd)
test_pd1=pd.get_dummies(test_pd1)

test_pd=np.array(test_pd)
test_pd1=np.array(test_pd1)

X_test=test_pd[:,:-32]
y_test=test_pd1[:,-5:]

with tf.Session() as sess:
  init.run()
  
  prediction=np.argmax(main_defend.eval(feed_dict={X:X_test}),axis=-1)

prediction

score=0

for i in range(len(prediction)):
  if y_test[i][prediction[i]]==1:
    score+=1

print(float(score/125973))

len(y_test)

